services:
  llama_server:
    build:
      context: .
      dockerfile: Dockerfile 
    ports:
      - "8000:8000"   
    deploy:
      resources:
        reservations:
          devices:
          -  driver: nvidia
             count: all
             capabilities: [gpu]
